{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from scipy.stats import iqr\n",
    "from statsmodels.tsa.deterministic import CalendarFourier\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel('Train.xlsx')\n",
    "test_df = pd.read_excel('Test.xlsx')\n",
    "sub_df = pd.read_csv('Submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all let's extract some features from datetime to facilitate future calculations: otherwise I will be forced to extract the same features for each split of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['dayofweek'] = train_df['Datetime'].apply(lambda x: x.dayofweek)\n",
    "train_df['weekofyear'] = train_df['Datetime'].apply(lambda x: x.isocalendar()[1])\n",
    "# train_df['weekofyear'] = train_df['weekofyear'].apply(int)\n",
    "train_df['dayofyear'] = train_df['Datetime'].dt.dayofyear\n",
    "\n",
    "test_df['dayofweek'] = test_df['Datetime'].apply(lambda x: x.dayofweek)\n",
    "test_df['weekofyear'] = test_df['Datetime'].apply(lambda x: x.isocalendar()[1])\n",
    "# test_df['weekofyear'] = test_df['weekofyear'].apply(int)\n",
    "test_df['dayofyear'] = test_df['Datetime'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find missing dates in the train data: the code comes from eda.ipynb notebook, but is self-explanatory too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occasional: 97 missing dates\n",
      "subscribed: 50 missing dates\n"
     ]
    }
   ],
   "source": [
    "train_full_dates = pd.date_range(start='2013-01-01', end='2016-06-01', freq='60min')[:-1]\n",
    "\n",
    "missing_dates_dict = {}\n",
    "for user_type in train_df['User_type'].unique():\n",
    "    train_date = train_df[train_df['User_type']==user_type]['Datetime'].tolist()\n",
    "    missing_dates = [i for i in train_full_dates if i not in train_date]\n",
    "    missing_dates_dict[user_type] = missing_dates\n",
    "    print(f\"{user_type}: {len(missing_dates)} missing dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's separate train and test data by user type as there will be a model for each user type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29878, 10) (29831, 10)\n"
     ]
    }
   ],
   "source": [
    "train_df.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "train_df_s = train_df[train_df['User_type']=='subscribed'].reset_index(drop=True)\n",
    "train_df_s.sort_values('Datetime', ascending=True, inplace=True)\n",
    "train_df_o = train_df[train_df['User_type']=='occasional'].reset_index(drop=True)\n",
    "train_df_o.sort_values('Datetime', ascending=True, inplace=True)\n",
    "\n",
    "print(train_df_s.shape, train_df_o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 10) (720, 10)\n"
     ]
    }
   ],
   "source": [
    "test_df.rename(\n",
    "    columns={'Unnamed: 0': 'index'},\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "test_df_s = test_df[test_df['User_type']=='subscribed'].reset_index(drop=True)\n",
    "test_df_s.sort_values('Datetime', ascending=True, inplace=True)\n",
    "test_df_o = test_df[test_df['User_type']=='occasional'].reset_index(drop=True)\n",
    "test_df_o.sort_values('Datetime', ascending=True, inplace=True)\n",
    "\n",
    "print(test_df_s.shape, test_df_o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing last 720 (this is the length of the test set) values as a validation set for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 10) (29158, 10)\n"
     ]
    }
   ],
   "source": [
    "valid_df_s = train_df_s.iloc[-720:]\n",
    "train_df_s = train_df_s.iloc[:-720]\n",
    "print(valid_df_s.shape, train_df_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 10) (29111, 10)\n"
     ]
    }
   ],
   "source": [
    "valid_df_o = train_df_o.iloc[-720:]\n",
    "train_df_o = train_df_o.iloc[:-720]\n",
    "print(valid_df_o.shape, train_df_o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling missing dates and corresponding feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 10) (29208, 10)\n",
      "(720, 10) (29208, 10)\n"
     ]
    }
   ],
   "source": [
    "for user_type in train_df['User_type'].unique():\n",
    "\n",
    "    if user_type == 'occasional':\n",
    "        temp_train_df = train_df_o.copy()\n",
    "    else:\n",
    "        temp_train_df = train_df_s.copy()\n",
    "\n",
    "    train_dict = defaultdict(list)\n",
    "    valid_dict = defaultdict(list)\n",
    "\n",
    "    for missing_date in missing_dates_dict[user_type]:\n",
    "\n",
    "        if missing_date < valid_df_s.iloc[0]['Datetime']:\n",
    "            train_dict['Datetime'].append(missing_date)\n",
    "            train_dict['St_Hour'].append(missing_date.hour)\n",
    "            train_dict['St_Day'].append(missing_date.day)\n",
    "            train_dict['St_Month'].append(missing_date.month)\n",
    "            train_dict['St_Year'].append(missing_date.year)\n",
    "            train_dict['dayofweek'].append(missing_date.dayofweek)\n",
    "            train_dict['weekofyear'].append(missing_date.isocalendar()[1])\n",
    "            train_dict['dayofyear'].append(missing_date.dayofyear)\n",
    "            train_dict['User_type'].append(user_type)\n",
    "            temp_df = temp_train_df[\n",
    "                (temp_train_df['Datetime']<missing_date) & \n",
    "                (temp_train_df['St_Hour']==missing_date.hour)\n",
    "            ]\n",
    "            train_dict['Rental_Bicycles_Count'] = temp_df['Rental_Bicycles_Count'].median()\n",
    "        else:\n",
    "            valid_dict['Datetime'].append(missing_date)\n",
    "            valid_dict['St_Hour'].append(missing_date.hour)\n",
    "            valid_dict['St_Day'].append(missing_date.day)\n",
    "            valid_dict['St_Month'].append(missing_date.month)\n",
    "            valid_dict['St_Year'].append(missing_date.year)\n",
    "            valid_dict['dayofweek'].append(missing_date.dayofweek)\n",
    "            valid_dict['weekofyear'].append(missing_date.isocalendar()[1])\n",
    "            valid_dict['dayofyear'].append(missing_date.dayofyear)\n",
    "            valid_dict['User_type'].append(user_type)\n",
    "            valid_dict['Rental_Bicycles_Count'] = temp_train_df[temp_train_df['St_Hour']==missing_date.hour]['Rental_Bicycles_Count'].median()\n",
    "\n",
    "    if user_type == 'occasional':\n",
    "        temp_train_df_o = pd.DataFrame(train_dict)\n",
    "        temp_train_df_o = temp_train_df_o[train_df_o.columns]\n",
    "        train_df_o = pd.concat([train_df_o, temp_train_df_o], axis=0).reset_index(drop=True)\n",
    "        train_df_o.sort_values('Datetime', ascending=True, inplace=True)\n",
    "\n",
    "        if len(valid_dict) != 0:\n",
    "            temp_valid_df_o = pd.DataFrame(valid_dict)\n",
    "            temp_valid_df_o = temp_valid_df_o[valid_df_o.columns]\n",
    "            valid_df_o = pd.concat([valid_df_o, temp_valid_df_o], axis=0).reset_index(drop=True)\n",
    "            valid_df_o.sort_values('Datetime', ascending=True, inplace=True)\n",
    "    else:\n",
    "        temp_train_df_s = pd.DataFrame(train_dict)\n",
    "        temp_train_df_s = temp_train_df_s[train_df_s.columns]\n",
    "        train_df_s = pd.concat([train_df_s, temp_train_df_s], axis=0).reset_index(drop=True)\n",
    "        train_df_s.sort_values('Datetime', ascending=True, inplace=True)\n",
    "\n",
    "        if len(valid_dict) != 0:\n",
    "            temp_valid_df_s = pd.DataFrame(valid_dict)\n",
    "            temp_valid_df_s = temp_valid_df_s[valid_df_s.columns]\n",
    "            valid_df_s = pd.concat([valid_df_s, temp_valid_df_s], axis=0).reset_index(drop=True)\n",
    "            valid_df_s.sort_values('Datetime', ascending=True, inplace=True)\n",
    "\n",
    "print(valid_df_s.shape, train_df_s.shape)\n",
    "print(valid_df_o.shape, train_df_o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 29208 entries, 0 to 29157\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   St_Hour                29208 non-null  int64         \n",
      " 1   St_Day                 29208 non-null  int64         \n",
      " 2   St_Month               29208 non-null  int64         \n",
      " 3   St_Year                29208 non-null  int64         \n",
      " 4   User_type              29208 non-null  object        \n",
      " 5   Datetime               29208 non-null  datetime64[ns]\n",
      " 6   Rental_Bicycles_Count  29208 non-null  float64       \n",
      " 7   dayofweek              29208 non-null  int64         \n",
      " 8   weekofyear             29208 non-null  int64         \n",
      " 9   dayofyear              29208 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(7), object(1)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df_s.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 29208 entries, 0 to 29110\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   St_Hour                29208 non-null  int64         \n",
      " 1   St_Day                 29208 non-null  int64         \n",
      " 2   St_Month               29208 non-null  int64         \n",
      " 3   St_Year                29208 non-null  int64         \n",
      " 4   User_type              29208 non-null  object        \n",
      " 5   Datetime               29208 non-null  datetime64[ns]\n",
      " 6   Rental_Bicycles_Count  29208 non-null  float64       \n",
      " 7   dayofweek              29208 non-null  int64         \n",
      " 8   weekofyear             29208 non-null  int64         \n",
      " 9   dayofyear              29208 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(7), object(1)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df_o.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some dummy features which will take into account day of week, week of year, hour of day, and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 92) (29208, 92) (720, 92)\n"
     ]
    }
   ],
   "source": [
    "encoder_s = OneHotEncoder(drop='first')\n",
    "\n",
    "dummy_columns = ['St_Hour', 'St_Month', 'dayofweek', 'weekofyear']\n",
    "\n",
    "train_array_s_dummies = encoder_s.fit_transform(\n",
    "    train_df_s[dummy_columns]\n",
    ").toarray()\n",
    "\n",
    "train_s_columns = []\n",
    "\n",
    "for i in range(len(encoder_s.categories_)):\n",
    "    for j in encoder_s.categories_[i][1:]:\n",
    "        train_s_columns.append(dummy_columns[i]+'_'+str(j))\n",
    "\n",
    "train_df_s_dummies = pd.DataFrame(\n",
    "    train_array_s_dummies,\n",
    "    columns=train_s_columns\n",
    ")\n",
    "\n",
    "valid_array_s_dummies = encoder_s.transform(\n",
    "    valid_df_s[dummy_columns]\n",
    ").toarray()\n",
    "\n",
    "valid_df_s_dummies = pd.DataFrame(\n",
    "    valid_array_s_dummies,\n",
    "    columns=train_s_columns\n",
    ")\n",
    "\n",
    "test_array_s_dummies = encoder_s.transform(\n",
    "    test_df_s[dummy_columns]\n",
    ").toarray()\n",
    "\n",
    "test_df_s_dummies = pd.DataFrame(\n",
    "    test_array_s_dummies,\n",
    "    columns=train_s_columns\n",
    ")\n",
    "\n",
    "print(valid_df_s_dummies.shape, train_df_s_dummies.shape, test_df_s_dummies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 92) (29208, 92) (720, 92)\n"
     ]
    }
   ],
   "source": [
    "encoder_o = OneHotEncoder(drop='first')\n",
    "\n",
    "dummy_columns = ['St_Hour', 'St_Month', 'dayofweek', 'weekofyear']\n",
    "\n",
    "train_array_o_dummies = encoder_o.fit_transform(\n",
    "    train_df_o[dummy_columns]\n",
    ").toarray()\n",
    "\n",
    "train_o_columns = []\n",
    "\n",
    "for i in range(len(encoder_o.categories_)):\n",
    "    for j in encoder_o.categories_[i][1:]:\n",
    "        train_o_columns.append(dummy_columns[i]+'_'+str(j))\n",
    "\n",
    "train_df_o_dummies = pd.DataFrame(\n",
    "    train_array_o_dummies,\n",
    "    columns=train_o_columns\n",
    ")\n",
    "\n",
    "valid_array_o_dummies = encoder_o.transform(\n",
    "    valid_df_o[dummy_columns]\n",
    ").toarray()\n",
    "\n",
    "valid_df_o_dummies = pd.DataFrame(\n",
    "    valid_array_o_dummies,\n",
    "    columns=train_o_columns\n",
    ")\n",
    "\n",
    "test_array_o_dummies = encoder_o.transform(\n",
    "    test_df_o[dummy_columns]\n",
    ").toarray()\n",
    "\n",
    "test_df_o_dummies = pd.DataFrame(\n",
    "    test_array_o_dummies,\n",
    "    columns=train_o_columns\n",
    ")\n",
    "\n",
    "print(valid_df_o_dummies.shape, train_df_o_dummies.shape, test_df_o_dummies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating target enconding map dictionary for day of year column using mean, max, min, std, and median values of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayofyear_grouped_df_s = train_df_s.groupby('dayofyear')['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median', iqr])\n",
    "dayofyear_map_dict_s = dayofyear_grouped_df_s.to_dict()\n",
    "\n",
    "dayofyear_grouped_df_o = train_df_o.groupby('dayofyear')['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median', iqr])\n",
    "dayofyear_map_dict_o = dayofyear_grouped_df_o.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating target enconding map dictionary for month and day of month columns' combination using mean, max, min, std, and median values of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_day_grouped_df_s = train_df_s.groupby(\n",
    "    ['St_Month', 'St_Day'], as_index=False\n",
    ")['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median', iqr])\n",
    "month_day_grouped_df_s['index'] = list(month_day_grouped_df_s[['St_Month', 'St_Day']].itertuples(index=False, name=None))\n",
    "month_day_grouped_df_s.drop(columns=['St_Month', 'St_Day'], inplace=True)\n",
    "month_day_map_dict_s = month_day_grouped_df_s.set_index('index').to_dict()\n",
    "\n",
    "train_df_s['St_Month_Day'] = list(train_df_s[['St_Month', 'St_Day']].itertuples(index=False, name=None))\n",
    "valid_df_s['St_Month_Day'] = list(valid_df_s[['St_Month', 'St_Day']].itertuples(index=False, name=None))\n",
    "\n",
    "month_day_grouped_df_o = train_df_o.groupby(\n",
    "    ['St_Month', 'St_Day'], as_index=False\n",
    ")['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median', iqr])\n",
    "month_day_grouped_df_o['index'] = list(month_day_grouped_df_o[['St_Month', 'St_Day']].itertuples(index=False, name=None))\n",
    "month_day_grouped_df_o.drop(columns=['St_Month', 'St_Day'], inplace=True)\n",
    "month_day_map_dict_o = month_day_grouped_df_o.set_index('index').to_dict()\n",
    "\n",
    "train_df_o['St_Month_Day'] = list(train_df_o[['St_Month', 'St_Day']].itertuples(index=False, name=None))\n",
    "valid_df_o['St_Month_Day'] = list(valid_df_o[['St_Month', 'St_Day']].itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating target enconding map dictionary for St_Month column using mean, max, min, std, and median values of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_grouped_df_s = train_df_s.groupby('St_Month')['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median', iqr])\n",
    "month_map_dict_s = month_grouped_df_s.to_dict()\n",
    "\n",
    "month_grouped_df_o = train_df_o.groupby('St_Month')['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median', iqr])\n",
    "month_map_dict_o = month_grouped_df_o.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating target enconding map dictionary for St_Hour column using mean, max, min, std, and median values of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_grouped_df_s = train_df_s.groupby('St_Hour')['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median', iqr])\n",
    "hour_map_dict_s = hour_grouped_df_s.to_dict()\n",
    "\n",
    "hour_grouped_df_o = train_df_o.groupby('St_Hour')['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median', iqr])\n",
    "hour_map_dict_o = hour_grouped_df_o.to_dict()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating target enconding map dictionary for day of week column using mean, max, min, std, and median values of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayofweek_grouped_df_s = train_df_s.groupby('dayofweek')['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median', iqr])\n",
    "dayofweek_map_dict_s = dayofweek_grouped_df_s.to_dict()\n",
    "\n",
    "dayofweek_grouped_df_o = train_df_o.groupby('dayofweek')['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median', iqr])\n",
    "dayofweek_map_dict_o = dayofweek_grouped_df_o.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating target enconding map dictionary for week of year column using mean, max, min, std, and median values of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekofyear_grouped_df_s = train_df_s.groupby('weekofyear')['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median', iqr])\n",
    "weekofyear_map_dict_s = weekofyear_grouped_df_s.to_dict()\n",
    "\n",
    "weekofyear_grouped_df_o = train_df_o.groupby('weekofyear')['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median', iqr])\n",
    "weekofyear_map_dict_o = weekofyear_grouped_df_o.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the map dictionaries to create the target encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['mean', 'max', 'min', 'std', 'median', 'iqr']:\n",
    "\n",
    "    train_df_s[f'dayofyear_{k}_te'] = train_df_s['dayofyear'].map(dayofyear_map_dict_s[k])\n",
    "    valid_df_s[f'dayofyear_{k}_te'] = valid_df_s['dayofyear'].map(dayofyear_map_dict_s[k])\n",
    "\n",
    "    train_df_o[f'dayofyear_{k}_te'] = train_df_o['dayofyear'].map(dayofyear_map_dict_o[k])\n",
    "    valid_df_o[f'dayofyear_{k}_te'] = valid_df_o['dayofyear'].map(dayofyear_map_dict_o[k])\n",
    "\n",
    "    train_df_s[f'month_day_{k}_te'] = train_df_s['St_Month_Day'].map(month_day_map_dict_s[k])\n",
    "    valid_df_s[f'month_day_{k}_te'] = valid_df_s['St_Month_Day'].map(month_day_map_dict_s[k])\n",
    "\n",
    "    train_df_o[f'month_day_{k}_te'] = train_df_o['St_Month_Day'].map(month_day_map_dict_o[k])\n",
    "    valid_df_o[f'month_day_{k}_te'] = valid_df_o['St_Month_Day'].map(month_day_map_dict_o[k])\n",
    "\n",
    "    train_df_s[f'month_{k}_te'] = train_df_s['St_Month'].map(month_map_dict_s[k])\n",
    "    valid_df_s[f'month_{k}_te'] = valid_df_s['St_Month'].map(month_map_dict_s[k])\n",
    "\n",
    "    train_df_o[f'month_{k}_te'] = train_df_o['St_Month'].map(month_map_dict_o[k])\n",
    "    valid_df_o[f'month_{k}_te'] = valid_df_o['St_Month'].map(month_map_dict_o[k])\n",
    "\n",
    "    train_df_s[f'hour_{k}_te'] = train_df_s['St_Hour'].map(hour_map_dict_s[k])\n",
    "    valid_df_s[f'hour_{k}_te'] = valid_df_s['St_Hour'].map(hour_map_dict_s[k])\n",
    "\n",
    "    train_df_o[f'hour_{k}_te'] = train_df_o['St_Hour'].map(hour_map_dict_o[k])\n",
    "    valid_df_o[f'hour_{k}_te'] = valid_df_o['St_Hour'].map(hour_map_dict_o[k])\n",
    "\n",
    "    train_df_s[f'dayofweek_{k}_te'] = train_df_s['dayofweek'].map(dayofweek_map_dict_s[k])\n",
    "    valid_df_s[f'dayofweek_{k}_te'] = valid_df_s['dayofweek'].map(dayofweek_map_dict_s[k])\n",
    "\n",
    "    train_df_o[f'dayofweek_{k}_te'] = train_df_o['dayofweek'].map(dayofweek_map_dict_o[k])\n",
    "    valid_df_o[f'dayofweek_{k}_te'] = valid_df_o['dayofweek'].map(dayofweek_map_dict_o[k])\n",
    "\n",
    "    train_df_s[f'weekofyear_{k}_te'] = train_df_s['weekofyear'].map(weekofyear_map_dict_s[k])\n",
    "    valid_df_s[f'weekofyear_{k}_te'] = valid_df_s['weekofyear'].map(weekofyear_map_dict_s[k])\n",
    "\n",
    "    train_df_o[f'weekofyear_{k}_te'] = train_df_o['weekofyear'].map(weekofyear_map_dict_o[k])\n",
    "    valid_df_o[f'weekofyear_{k}_te'] = valid_df_o['weekofyear'].map(weekofyear_map_dict_o[k])\n",
    "\n",
    "redundant_columns = [\n",
    "    'St_Hour',  'St_Day', 'St_Month',\n",
    "    'St_Year', 'User_type','dayofweek', \n",
    "    'weekofyear','dayofyear', 'St_Month_Day'\n",
    "]\n",
    "\n",
    "train_df_s.drop(columns=redundant_columns, inplace=True)\n",
    "valid_df_s.drop(columns=redundant_columns, inplace=True)\n",
    "\n",
    "train_df_o.drop(columns=redundant_columns, inplace=True)\n",
    "valid_df_o.drop(columns=redundant_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also add lag features in the training data and concatenate the train datasets with the dummy datasets created for the training data. Moreover let's remove NaN values emerged due to computing lag values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    train_df_s[f'lag{i}_target'] = train_df_s['Rental_Bicycles_Count'].shift(i)\n",
    "    train_df_o[f'lag{i}_target'] = train_df_o['Rental_Bicycles_Count'].shift(i)\n",
    "\n",
    "train_df_s = pd.concat([train_df_s, train_df_s_dummies], axis=1)\n",
    "train_df_s.dropna(inplace=True)\n",
    "train_df_s.reset_index(inplace=True, drop=True)\n",
    "\n",
    "train_df_o = pd.concat([train_df_o, train_df_o_dummies], axis=1)\n",
    "train_df_o.dropna(inplace=True)\n",
    "train_df_o.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create time_step feature which will allow to estimate trend component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_s['time_step'] = np.arange(1, train_df_s.shape[0]+1)\n",
    "train_df_o['time_step'] = np.arange(1, train_df_o.shape[0]+1)\n",
    "\n",
    "valid_df_s['time_step'] = np.arange(train_df_s.shape[0]+1, valid_df_s.shape[0]+train_df_s.shape[0]+1)\n",
    "valid_df_o['time_step'] = np.arange(train_df_o.shape[0]+1, valid_df_o.shape[0]+train_df_o.shape[0]+1)\n",
    "\n",
    "test_df_s['time_step'] = np.arange(valid_df_s.shape[0]+train_df_s.shape[0]+1, test_df_s.shape[0]+valid_df_s.shape[0]+train_df_s.shape[0]+1)\n",
    "test_df_o['time_step'] = np.arange(valid_df_o.shape[0]+train_df_o.shape[0]+1, test_df_o.shape[0]+valid_df_o.shape[0]+train_df_o.shape[0]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's derive the Fourier series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720 29198 720\n"
     ]
    }
   ],
   "source": [
    "FT_PERIODS = 1\n",
    "\n",
    "cal_fourier_gen_s = CalendarFourier(\"YE\", FT_PERIODS)\n",
    "index = pd.DatetimeIndex(train_df_s['Datetime'])\n",
    "fs_train_df_s = cal_fourier_gen_s.in_sample(index)\n",
    "fs_train_df_s.columns = [\n",
    "    f'sin{int(i/2)+1}'\n",
    "    if i%2==0 else f'cos{int(i/2)+1}'\n",
    "    for i in range(FT_PERIODS*2)\n",
    "]\n",
    "fs_train_df_s.reset_index(inplace=True)\n",
    "fs_train_df_s.rename(columns={'index': 'Datetime'}, inplace=True)\n",
    "\n",
    "out_of_sample_periods = 2*valid_df_s.shape[0]\n",
    "fs_valid_df_s = cal_fourier_gen_s.out_of_sample(\n",
    "    steps=out_of_sample_periods,\n",
    "    index=pd.date_range(\n",
    "        start=train_df_s.iloc[0]['Datetime'].strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        periods=train_df_s.shape[0],\n",
    "        freq='60min'\n",
    "    )\n",
    ")\n",
    "fs_valid_df_s.columns = [\n",
    "    f'sin{int(i/2)+1}'\n",
    "    if i%2==0 else f'cos{int(i/2)+1}'\n",
    "    for i in range(FT_PERIODS*2)\n",
    "]\n",
    "fs_valid_df_s.reset_index(inplace=True)\n",
    "fs_valid_df_s.rename(columns={'index': 'Datetime'}, inplace=True)\n",
    "\n",
    "fs_test_df_s = fs_valid_df_s.iloc[-720:]\n",
    "fs_valid_df_s = fs_valid_df_s.iloc[:720]\n",
    "\n",
    "print(fs_valid_df_s.shape[0], fs_train_df_s.shape[0], fs_test_df_s.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720 29198 720\n"
     ]
    }
   ],
   "source": [
    "FT_PERIODS = 1\n",
    "\n",
    "cal_fourier_gen_o = CalendarFourier(\"YE\", FT_PERIODS)\n",
    "index = pd.DatetimeIndex(train_df_o['Datetime'])\n",
    "fs_train_df_o = cal_fourier_gen_o.in_sample(index)\n",
    "fs_train_df_o.columns = [\n",
    "    f'sin{int(i/2)+1}'\n",
    "    if i%2==0 else f'cos{int(i/2)+1}'\n",
    "    for i in range(FT_PERIODS*2)\n",
    "]\n",
    "fs_train_df_o.reset_index(inplace=True)\n",
    "fs_train_df_o.rename(columns={'index': 'Datetime'}, inplace=True)\n",
    "\n",
    "out_of_sample_periods = valid_df_o.shape[0]\n",
    "fs_valid_df_o = cal_fourier_gen_o.out_of_sample(\n",
    "    steps=out_of_sample_periods,\n",
    "    index=pd.date_range(\n",
    "        start=train_df_o.iloc[0]['Datetime'].strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        periods=train_df_o.shape[0],\n",
    "        freq='60min'\n",
    "    )\n",
    ")\n",
    "fs_valid_df_o.columns = [\n",
    "    f'sin{int(i/2)+1}'\n",
    "    if i%2==0 else f'cos{int(i/2)+1}'\n",
    "    for i in range(FT_PERIODS*2)\n",
    "]\n",
    "fs_valid_df_o.reset_index(inplace=True)\n",
    "fs_valid_df_o.rename(columns={'index': 'Datetime'}, inplace=True)\n",
    "\n",
    "fs_test_df_o = fs_valid_df_o.iloc[-720:]\n",
    "fs_valid_df_o = fs_valid_df_o.iloc[:720]\n",
    "\n",
    "print(fs_valid_df_o.shape[0], fs_train_df_o.shape[0], fs_test_df_o.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging and concatenating all available features for train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_s = pd.merge(train_df_s, fs_train_df_s, on='Datetime')\n",
    "train_df_s.drop(columns='Datetime', inplace=True)\n",
    "\n",
    "valid_df_s = pd.merge(valid_df_s, fs_valid_df_s, on='Datetime')\n",
    "valid_df_s = pd.concat([valid_df_s, valid_df_s_dummies], axis=1)\n",
    "valid_df_s.drop(columns='Datetime', inplace=True)\n",
    "\n",
    "train_df_o = pd.merge(train_df_o, fs_train_df_o, on='Datetime')\n",
    "train_df_o.drop(columns='Datetime', inplace=True)\n",
    "\n",
    "valid_df_o = pd.merge(valid_df_o, fs_valid_df_o, on='Datetime')\n",
    "valid_df_o = pd.concat([valid_df_o, valid_df_o_dummies], axis=1)\n",
    "valid_df_o.drop(columns='Datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the derived train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rental_Bicycles_Count</th>\n",
       "      <th>time_step</th>\n",
       "      <th>dayofyear_mean_te</th>\n",
       "      <th>month_day_mean_te</th>\n",
       "      <th>month_mean_te</th>\n",
       "      <th>hour_mean_te</th>\n",
       "      <th>dayofweek_mean_te</th>\n",
       "      <th>weekofyear_mean_te</th>\n",
       "      <th>dayofyear_max_te</th>\n",
       "      <th>month_day_max_te</th>\n",
       "      <th>...</th>\n",
       "      <th>weekofyear_46</th>\n",
       "      <th>weekofyear_47</th>\n",
       "      <th>weekofyear_48</th>\n",
       "      <th>weekofyear_49</th>\n",
       "      <th>weekofyear_50</th>\n",
       "      <th>weekofyear_51</th>\n",
       "      <th>weekofyear_52</th>\n",
       "      <th>weekofyear_53</th>\n",
       "      <th>sin1</th>\n",
       "      <th>cos1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56.0</td>\n",
       "      <td>11</td>\n",
       "      <td>90.291667</td>\n",
       "      <td>90.291667</td>\n",
       "      <td>156.010417</td>\n",
       "      <td>263.433854</td>\n",
       "      <td>258.544301</td>\n",
       "      <td>136.574074</td>\n",
       "      <td>344.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007173</td>\n",
       "      <td>0.999974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.0</td>\n",
       "      <td>12</td>\n",
       "      <td>90.291667</td>\n",
       "      <td>90.291667</td>\n",
       "      <td>156.010417</td>\n",
       "      <td>297.797042</td>\n",
       "      <td>258.544301</td>\n",
       "      <td>136.574074</td>\n",
       "      <td>344.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007890</td>\n",
       "      <td>0.999969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.0</td>\n",
       "      <td>13</td>\n",
       "      <td>90.291667</td>\n",
       "      <td>90.291667</td>\n",
       "      <td>156.010417</td>\n",
       "      <td>342.973706</td>\n",
       "      <td>258.544301</td>\n",
       "      <td>136.574074</td>\n",
       "      <td>344.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008607</td>\n",
       "      <td>0.999963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>14</td>\n",
       "      <td>90.291667</td>\n",
       "      <td>90.291667</td>\n",
       "      <td>156.010417</td>\n",
       "      <td>340.710764</td>\n",
       "      <td>258.544301</td>\n",
       "      <td>136.574074</td>\n",
       "      <td>344.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009324</td>\n",
       "      <td>0.999957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54.0</td>\n",
       "      <td>15</td>\n",
       "      <td>90.291667</td>\n",
       "      <td>90.291667</td>\n",
       "      <td>156.010417</td>\n",
       "      <td>341.898110</td>\n",
       "      <td>258.544301</td>\n",
       "      <td>136.574074</td>\n",
       "      <td>344.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010041</td>\n",
       "      <td>0.999950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rental_Bicycles_Count  time_step  dayofyear_mean_te  month_day_mean_te  \\\n",
       "0                   56.0         11          90.291667          90.291667   \n",
       "1                   40.0         12          90.291667          90.291667   \n",
       "2                   38.0         13          90.291667          90.291667   \n",
       "3                   35.0         14          90.291667          90.291667   \n",
       "4                   54.0         15          90.291667          90.291667   \n",
       "\n",
       "   month_mean_te  hour_mean_te  dayofweek_mean_te  weekofyear_mean_te  \\\n",
       "0     156.010417    263.433854         258.544301          136.574074   \n",
       "1     156.010417    297.797042         258.544301          136.574074   \n",
       "2     156.010417    342.973706         258.544301          136.574074   \n",
       "3     156.010417    340.710764         258.544301          136.574074   \n",
       "4     156.010417    341.898110         258.544301          136.574074   \n",
       "\n",
       "   dayofyear_max_te  month_day_max_te  ...  weekofyear_46  weekofyear_47  \\\n",
       "0             344.0             344.0  ...            0.0            0.0   \n",
       "1             344.0             344.0  ...            0.0            0.0   \n",
       "2             344.0             344.0  ...            0.0            0.0   \n",
       "3             344.0             344.0  ...            0.0            0.0   \n",
       "4             344.0             344.0  ...            0.0            0.0   \n",
       "\n",
       "   weekofyear_48  weekofyear_49  weekofyear_50  weekofyear_51  weekofyear_52  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   weekofyear_53      sin1      cos1  \n",
       "0            0.0  0.007173  0.999974  \n",
       "1            0.0  0.007890  0.999969  \n",
       "2            0.0  0.008607  0.999963  \n",
       "3            0.0  0.009324  0.999957  \n",
       "4            0.0  0.010041  0.999950  \n",
       "\n",
       "[5 rows x 142 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_s.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rental_Bicycles_Count</th>\n",
       "      <th>time_step</th>\n",
       "      <th>dayofyear_mean_te</th>\n",
       "      <th>month_day_mean_te</th>\n",
       "      <th>month_mean_te</th>\n",
       "      <th>hour_mean_te</th>\n",
       "      <th>dayofweek_mean_te</th>\n",
       "      <th>weekofyear_mean_te</th>\n",
       "      <th>dayofyear_max_te</th>\n",
       "      <th>month_day_max_te</th>\n",
       "      <th>...</th>\n",
       "      <th>weekofyear_46</th>\n",
       "      <th>weekofyear_47</th>\n",
       "      <th>weekofyear_48</th>\n",
       "      <th>weekofyear_49</th>\n",
       "      <th>weekofyear_50</th>\n",
       "      <th>weekofyear_51</th>\n",
       "      <th>weekofyear_52</th>\n",
       "      <th>weekofyear_53</th>\n",
       "      <th>sin1</th>\n",
       "      <th>cos1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.0</td>\n",
       "      <td>11</td>\n",
       "      <td>77.4375</td>\n",
       "      <td>77.4375</td>\n",
       "      <td>63.415659</td>\n",
       "      <td>188.577650</td>\n",
       "      <td>161.138889</td>\n",
       "      <td>77.013889</td>\n",
       "      <td>439.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007173</td>\n",
       "      <td>0.999974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.0</td>\n",
       "      <td>12</td>\n",
       "      <td>77.4375</td>\n",
       "      <td>77.4375</td>\n",
       "      <td>63.415659</td>\n",
       "      <td>225.508628</td>\n",
       "      <td>161.138889</td>\n",
       "      <td>77.013889</td>\n",
       "      <td>439.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007890</td>\n",
       "      <td>0.999969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.0</td>\n",
       "      <td>13</td>\n",
       "      <td>77.4375</td>\n",
       "      <td>77.4375</td>\n",
       "      <td>63.415659</td>\n",
       "      <td>257.465900</td>\n",
       "      <td>161.138889</td>\n",
       "      <td>77.013889</td>\n",
       "      <td>439.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008607</td>\n",
       "      <td>0.999963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.0</td>\n",
       "      <td>14</td>\n",
       "      <td>77.4375</td>\n",
       "      <td>77.4375</td>\n",
       "      <td>63.415659</td>\n",
       "      <td>273.066557</td>\n",
       "      <td>161.138889</td>\n",
       "      <td>77.013889</td>\n",
       "      <td>439.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009324</td>\n",
       "      <td>0.999957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.0</td>\n",
       "      <td>15</td>\n",
       "      <td>77.4375</td>\n",
       "      <td>77.4375</td>\n",
       "      <td>63.415659</td>\n",
       "      <td>287.704191</td>\n",
       "      <td>161.138889</td>\n",
       "      <td>77.013889</td>\n",
       "      <td>439.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010041</td>\n",
       "      <td>0.999950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rental_Bicycles_Count  time_step  dayofyear_mean_te  month_day_mean_te  \\\n",
       "0                   29.0         11            77.4375            77.4375   \n",
       "1                   16.0         12            77.4375            77.4375   \n",
       "2                   21.0         13            77.4375            77.4375   \n",
       "3                   30.0         14            77.4375            77.4375   \n",
       "4                   39.0         15            77.4375            77.4375   \n",
       "\n",
       "   month_mean_te  hour_mean_te  dayofweek_mean_te  weekofyear_mean_te  \\\n",
       "0      63.415659    188.577650         161.138889           77.013889   \n",
       "1      63.415659    225.508628         161.138889           77.013889   \n",
       "2      63.415659    257.465900         161.138889           77.013889   \n",
       "3      63.415659    273.066557         161.138889           77.013889   \n",
       "4      63.415659    287.704191         161.138889           77.013889   \n",
       "\n",
       "   dayofyear_max_te  month_day_max_te  ...  weekofyear_46  weekofyear_47  \\\n",
       "0             439.0             439.0  ...            0.0            0.0   \n",
       "1             439.0             439.0  ...            0.0            0.0   \n",
       "2             439.0             439.0  ...            0.0            0.0   \n",
       "3             439.0             439.0  ...            0.0            0.0   \n",
       "4             439.0             439.0  ...            0.0            0.0   \n",
       "\n",
       "   weekofyear_48  weekofyear_49  weekofyear_50  weekofyear_51  weekofyear_52  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   weekofyear_53      sin1      cos1  \n",
       "0            0.0  0.007173  0.999974  \n",
       "1            0.0  0.007890  0.999969  \n",
       "2            0.0  0.008607  0.999963  \n",
       "3            0.0  0.009324  0.999957  \n",
       "4            0.0  0.010041  0.999950  \n",
       "\n",
       "[5 rows x 142 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_o.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_o.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in ['mean', 'max', 'min', 'std', 'median', 'iqr']:\n",
    "\n",
    "#     train_df_s[f'dayofyear_{k}_te'] = train_df_s['dayofyear'].map(dayofyear_map_dict_s[k])\n",
    "#     train_df_o[f'dayofyear_{k}_te'] = train_df_o['dayofyear'].map(dayofyear_map_dict_o[k])\n",
    "\n",
    "#     train_df_s[f'month_day_{k}_te'] = train_df_s['St_Month_Day'].map(month_day_map_dict_s[k])\n",
    "#     train_df_o[f'month_day_{k}_te'] = train_df_o['St_Month_Day'].map(month_day_map_dict_o[k])\n",
    "\n",
    "#     train_df_s[f'month_{k}_te'] = train_df_s['St_Month'].map(month_map_dict_s[k])\n",
    "#     train_df_o[f'month_{k}_te'] = train_df_o['St_Month'].map(month_map_dict_o[k])\n",
    "\n",
    "#     train_df_s[f'hour_{k}_te'] = train_df_s['St_Hour'].map(hour_map_dict_s[k])\n",
    "#     train_df_o[f'hour_{k}_te'] = train_df_o['St_Hour'].map(hour_map_dict_o[k])\n",
    "\n",
    "#     train_df_s[f'dayofweek_{k}_te'] = train_df_s['dayofweek'].map(dayofweek_map_dict_s[k])\n",
    "#     train_df_o[f'dayofweek_{k}_te'] = train_df_o['dayofweek'].map(dayofweek_map_dict_o[k])\n",
    "\n",
    "#     train_df_s[f'weekofyear_{k}_te'] = train_df_s['weekofyear'].map(weekofyear_map_dict_s[k])\n",
    "#     train_df_o[f'weekofyear_{k}_te'] = train_df_o['weekofyear'].map(weekofyear_map_dict_o[k])\n",
    "\n",
    "# redundant_columns = [\n",
    "#     'St_Hour',  'St_Day', 'St_Month',\n",
    "#     'St_Year', 'User_type','dayofweek', \n",
    "#     'weekofyear','dayofyear', 'St_Month_Day'\n",
    "# ]\n",
    "\n",
    "# train_df_s.drop(columns=redundant_columns, inplace=True)\n",
    "# train_df_s = pd.merge(train_df_s, fs_train_df_s, on='Datetime')\n",
    "# train_df_s.drop(columns='Datetime', inplace=True)\n",
    "\n",
    "# train_df_o.drop(columns=redundant_columns, inplace=True)\n",
    "# train_df_o = pd.merge(train_df_o, fs_train_df_o, on='Datetime')\n",
    "# train_df_o.drop(columns='Datetime', inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
