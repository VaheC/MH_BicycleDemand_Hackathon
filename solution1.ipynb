{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel('Train.xlsx')\n",
    "test_df = pd.read_excel('Test.xlsx')\n",
    "sub_df = pd.read_csv('Submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all let's extract some features from datetime to facilitate future calculations: otherwise I will be forced to extract the same features for each split of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['dayofweek'] = train_df['Datetime'].apply(lambda x: x.dayofweek)\n",
    "train_df['weekofyear'] = train_df['Datetime'].apply(lambda x: x.isocalendar()[1])\n",
    "# train_df['weekofyear'] = train_df['weekofyear'].apply(int)\n",
    "train_df['dayofyear'] = train_df['Datetime'].dt.dayofyear\n",
    "\n",
    "test_df['dayofweek'] = test_df['Datetime'].apply(lambda x: x.dayofweek)\n",
    "test_df['weekofyear'] = test_df['Datetime'].apply(lambda x: x.isocalendar()[1])\n",
    "# test_df['weekofyear'] = test_df['weekofyear'].apply(int)\n",
    "test_df['dayofyear'] = test_df['Datetime'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find missing dates in the train data: the code comes from eda.ipynb notebook, but is self-explanatory too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occasional: 97 missing dates\n",
      "subscribed: 50 missing dates\n"
     ]
    }
   ],
   "source": [
    "train_full_dates = pd.date_range(start='2013-01-01', end='2016-06-01', freq='60min')[:-1]\n",
    "\n",
    "missing_dates_dict = {}\n",
    "for user_type in train_df['User_type'].unique():\n",
    "    train_date = train_df[train_df['User_type']==user_type]['Datetime'].tolist()\n",
    "    missing_dates = [i for i in train_full_dates if i not in train_date]\n",
    "    missing_dates_dict[user_type] = missing_dates\n",
    "    print(f\"{user_type}: {len(missing_dates)} missing dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's separate train and test data by user type as there will be a model for each user type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29878, 10) (29831, 10)\n"
     ]
    }
   ],
   "source": [
    "train_df.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "train_df_s = train_df[train_df['User_type']=='subscribed'].reset_index(drop=True)\n",
    "train_df_s.sort_values('Datetime', ascending=True, inplace=True)\n",
    "train_df_o = train_df[train_df['User_type']=='occasional'].reset_index(drop=True)\n",
    "train_df_o.sort_values('Datetime', ascending=True, inplace=True)\n",
    "\n",
    "print(train_df_s.shape, train_df_o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 10) (720, 10)\n"
     ]
    }
   ],
   "source": [
    "test_df.rename(\n",
    "    columns={'Unnamed: 0': 'index'},\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "test_df_s = test_df[test_df['User_type']=='subscribed'].reset_index(drop=True)\n",
    "test_df_s.sort_values('Datetime', ascending=True, inplace=True)\n",
    "test_df_o = test_df[test_df['User_type']=='occasional'].reset_index(drop=True)\n",
    "test_df_o.sort_values('Datetime', ascending=True, inplace=True)\n",
    "\n",
    "print(test_df_s.shape, test_df_o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing last 720 (this is the length of the test set) values as a validation set for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 10) (29158, 10)\n"
     ]
    }
   ],
   "source": [
    "valid_df_s = train_df_s.iloc[-720:]\n",
    "train_df_s = train_df_s.iloc[:-720]\n",
    "print(valid_df_s.shape, train_df_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 10) (29111, 10)\n"
     ]
    }
   ],
   "source": [
    "valid_df_o = train_df_o.iloc[-720:]\n",
    "train_df_o = train_df_o.iloc[:-720]\n",
    "print(valid_df_o.shape, train_df_o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling missing dates and corresponding feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 10) (29208, 10)\n",
      "(720, 10) (29208, 10)\n"
     ]
    }
   ],
   "source": [
    "for user_type in train_df['User_type'].unique():\n",
    "\n",
    "    if user_type == 'occasional':\n",
    "        temp_train_df = train_df_o.copy()\n",
    "    else:\n",
    "        temp_train_df = train_df_s.copy()\n",
    "\n",
    "    train_dict = defaultdict(list)\n",
    "    valid_dict = defaultdict(list)\n",
    "\n",
    "    for missing_date in missing_dates_dict[user_type]:\n",
    "\n",
    "        if missing_date < valid_df_s.iloc[0]['Datetime']:\n",
    "            train_dict['Datetime'].append(missing_date)\n",
    "            train_dict['St_Hour'].append(missing_date.hour)\n",
    "            train_dict['St_Day'].append(missing_date.day)\n",
    "            train_dict['St_Month'].append(missing_date.month)\n",
    "            train_dict['St_Year'].append(missing_date.year)\n",
    "            train_dict['dayofweek'].append(missing_date.dayofweek)\n",
    "            train_dict['weekofyear'].append(missing_date.isocalendar()[1])\n",
    "            train_dict['dayofyear'].append(missing_date.dayofyear)\n",
    "            train_dict['User_type'].append(user_type)\n",
    "            temp_df = temp_train_df[\n",
    "                (temp_train_df['Datetime']<missing_date) & \n",
    "                (temp_train_df['St_Hour']==missing_date.hour)\n",
    "            ]\n",
    "            train_dict['Rental_Bicycles_Count'] = temp_df['Rental_Bicycles_Count'].median()\n",
    "        else:\n",
    "            valid_dict['Datetime'].append(missing_date)\n",
    "            valid_dict['St_Hour'].append(missing_date.hour)\n",
    "            valid_dict['St_Day'].append(missing_date.day)\n",
    "            valid_dict['St_Month'].append(missing_date.month)\n",
    "            valid_dict['St_Year'].append(missing_date.year)\n",
    "            valid_dict['dayofweek'].append(missing_date.dayofweek)\n",
    "            valid_dict['weekofyear'].append(missing_date.isocalendar()[1])\n",
    "            valid_dict['dayofyear'].append(missing_date.dayofyear)\n",
    "            valid_dict['User_type'].append(user_type)\n",
    "            valid_dict['Rental_Bicycles_Count'] = temp_train_df[temp_train_df['St_Hour']==missing_date.hour]['Rental_Bicycles_Count'].median()\n",
    "\n",
    "    if user_type == 'occasional':\n",
    "        temp_train_df_o = pd.DataFrame(train_dict)\n",
    "        temp_train_df_o = temp_train_df_o[train_df_o.columns]\n",
    "        train_df_o = pd.concat([train_df_o, temp_train_df_o], axis=0).reset_index(drop=True)\n",
    "        train_df_o.sort_values('Datetime', ascending=True, inplace=True)\n",
    "\n",
    "        if len(valid_dict) != 0:\n",
    "            temp_valid_df_o = pd.DataFrame(valid_dict)\n",
    "            temp_valid_df_o = temp_valid_df_o[valid_df_o.columns]\n",
    "            valid_df_o = pd.concat([valid_df_o, temp_valid_df_o], axis=0).reset_index(drop=True)\n",
    "            valid_df_o.sort_values('Datetime', ascending=True, inplace=True)\n",
    "    else:\n",
    "        temp_train_df_s = pd.DataFrame(train_dict)\n",
    "        temp_train_df_s = temp_train_df_s[train_df_s.columns]\n",
    "        train_df_s = pd.concat([train_df_s, temp_train_df_s], axis=0).reset_index(drop=True)\n",
    "        train_df_s.sort_values('Datetime', ascending=True, inplace=True)\n",
    "\n",
    "        if len(valid_dict) != 0:\n",
    "            temp_valid_df_s = pd.DataFrame(valid_dict)\n",
    "            temp_valid_df_s = temp_valid_df_s[valid_df_s.columns]\n",
    "            valid_df_s = pd.concat([valid_df_s, temp_valid_df_s], axis=0).reset_index(drop=True)\n",
    "            valid_df_s.sort_values('Datetime', ascending=True, inplace=True)\n",
    "\n",
    "print(valid_df_s.shape, train_df_s.shape)\n",
    "print(valid_df_o.shape, train_df_o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 29208 entries, 0 to 29157\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   St_Hour                29208 non-null  int64         \n",
      " 1   St_Day                 29208 non-null  int64         \n",
      " 2   St_Month               29208 non-null  int64         \n",
      " 3   St_Year                29208 non-null  int64         \n",
      " 4   User_type              29208 non-null  object        \n",
      " 5   Datetime               29208 non-null  datetime64[ns]\n",
      " 6   Rental_Bicycles_Count  29208 non-null  float64       \n",
      " 7   dayofweek              29208 non-null  int64         \n",
      " 8   weekofyear             29208 non-null  int64         \n",
      " 9   dayofyear              29208 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(7), object(1)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df_s.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 29208 entries, 0 to 29110\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   St_Hour                29208 non-null  int64         \n",
      " 1   St_Day                 29208 non-null  int64         \n",
      " 2   St_Month               29208 non-null  int64         \n",
      " 3   St_Year                29208 non-null  int64         \n",
      " 4   User_type              29208 non-null  object        \n",
      " 5   Datetime               29208 non-null  datetime64[ns]\n",
      " 6   Rental_Bicycles_Count  29208 non-null  float64       \n",
      " 7   dayofweek              29208 non-null  int64         \n",
      " 8   weekofyear             29208 non-null  int64         \n",
      " 9   dayofyear              29208 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(7), object(1)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df_o.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create step feature which will allow to estimate trend component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_s['step'] = np.arange(1, train_df_s.shape[0]+1)\n",
    "train_df_o['step'] = np.arange(1, train_df_o.shape[0]+1)\n",
    "\n",
    "valid_df_s['step'] = np.arange(train_df_s.shape[0]+1, valid_df_s.shape[0]+train_df_s.shape[0]+1)\n",
    "valid_df_o['step'] = np.arange(train_df_o.shape[0]+1, valid_df_o.shape[0]+train_df_o.shape[0]+1)\n",
    "\n",
    "test_df_s['step'] = np.arange(valid_df_s.shape[0]+train_df_s.shape[0]+1, test_df_s.shape[0]+valid_df_s.shape[0]+train_df_s.shape[0]+1)\n",
    "test_df_o['step'] = np.arange(valid_df_o.shape[0]+train_df_o.shape[0]+1, test_df_o.shape[0]+valid_df_o.shape[0]+train_df_o.shape[0]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some dummy features which will take into account day of week, week of year, hour of day, and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 92) (29208, 92)\n"
     ]
    }
   ],
   "source": [
    "encoder_s = OneHotEncoder(drop='first')\n",
    "\n",
    "dummy_columns = ['St_Hour', 'St_Month', 'dayofweek', 'weekofyear']\n",
    "\n",
    "train_array_s_dummies = encoder_s.fit_transform(\n",
    "    train_df_s[dummy_columns]\n",
    ").toarray()\n",
    "\n",
    "train_s_columns = []\n",
    "\n",
    "for i in range(len(encoder_s.categories_)):\n",
    "    for j in encoder_s.categories_[i][1:]:\n",
    "        train_s_columns.append(dummy_columns[i]+'_'+str(j))\n",
    "\n",
    "train_df_s_dummies = pd.DataFrame(\n",
    "    train_array_s_dummies,\n",
    "    columns=train_s_columns\n",
    ")\n",
    "\n",
    "valid_array_s_dummies = encoder_s.transform(\n",
    "    valid_df_s[dummy_columns]\n",
    ").toarray()\n",
    "\n",
    "valid_df_s_dummies = pd.DataFrame(\n",
    "    valid_array_s_dummies,\n",
    "    columns=train_s_columns\n",
    ")\n",
    "\n",
    "print(valid_df_s_dummies.shape, train_df_s_dummies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 92) (29208, 92)\n"
     ]
    }
   ],
   "source": [
    "encoder_o = OneHotEncoder(drop='first')\n",
    "\n",
    "dummy_columns = ['St_Hour', 'St_Month', 'dayofweek', 'weekofyear']\n",
    "\n",
    "train_array_o_dummies = encoder_o.fit_transform(\n",
    "    train_df_o[dummy_columns]\n",
    ").toarray()\n",
    "\n",
    "train_o_columns = []\n",
    "\n",
    "for i in range(len(encoder_o.categories_)):\n",
    "    for j in encoder_o.categories_[i][1:]:\n",
    "        train_o_columns.append(dummy_columns[i]+'_'+str(j))\n",
    "\n",
    "train_df_o_dummies = pd.DataFrame(\n",
    "    train_array_o_dummies,\n",
    "    columns=train_o_columns\n",
    ")\n",
    "\n",
    "valid_array_o_dummies = encoder_o.transform(\n",
    "    valid_df_o[dummy_columns]\n",
    ").toarray()\n",
    "\n",
    "valid_df_o_dummies = pd.DataFrame(\n",
    "    valid_array_o_dummies,\n",
    "    columns=train_o_columns\n",
    ")\n",
    "\n",
    "print(valid_df_o_dummies.shape, train_df_o_dummies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating target enconding map dictionary for day of year column using mean, max, min, std, and median values of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayofyear_grouped_df_s = train_df_s.groupby('dayofyear')['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median'])\n",
    "dayofyear_map_dict_s = dayofyear_grouped_df_s.to_dict()\n",
    "for k in ['mean', 'max', 'min', 'std', 'median']:\n",
    "    train_df_s[f'dayofyear_{k}_te'] = train_df_s['dayofyear'].map(dayofyear_map_dict_s[k])\n",
    "    valid_df_s[f'dayofyear_{k}_te'] = valid_df_s['dayofyear'].map(dayofyear_map_dict_s[k])\n",
    "\n",
    "dayofyear_grouped_df_o = train_df_o.groupby('dayofyear')['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median'])\n",
    "dayofyear_map_dict_o = dayofyear_grouped_df_o.to_dict()\n",
    "for k in ['mean', 'max', 'min', 'std', 'median']:\n",
    "    train_df_o[f'dayofyear_{k}_te'] = train_df_o['dayofyear'].map(dayofyear_map_dict_o[k])\n",
    "    valid_df_o[f'dayofyear_{k}_te'] = valid_df_o['dayofyear'].map(dayofyear_map_dict_o[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating target enconding map dictionary for month and day of month columns' combination using mean, max, min, std, and median values of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_day_grouped_df_s = train_df_s.groupby(\n",
    "    ['St_Month', 'St_Day'], as_index=False\n",
    ")['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median'])\n",
    "month_day_grouped_df_s['index'] = list(month_day_grouped_df_s[['St_Month', 'St_Day']].itertuples(index=False, name=None))\n",
    "month_day_grouped_df_s.drop(columns=['St_Month', 'St_Day'], inplace=True)\n",
    "month_day_map_dict_s = month_day_grouped_df_s.set_index('index').to_dict()\n",
    "\n",
    "train_df_s['St_Month_Day'] = list(train_df_s[['St_Month', 'St_Day']].itertuples(index=False, name=None))\n",
    "valid_df_s['St_Month_Day'] = list(valid_df_s[['St_Month', 'St_Day']].itertuples(index=False, name=None))\n",
    "\n",
    "month_day_grouped_df_o = train_df_o.groupby(\n",
    "    ['St_Month', 'St_Day'], as_index=False\n",
    ")['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median'])\n",
    "month_day_grouped_df_o['index'] = list(month_day_grouped_df_o[['St_Month', 'St_Day']].itertuples(index=False, name=None))\n",
    "month_day_grouped_df_o.drop(columns=['St_Month', 'St_Day'], inplace=True)\n",
    "month_day_map_dict_o = month_day_grouped_df_o.set_index('index').to_dict()\n",
    "\n",
    "train_df_o['St_Month_Day'] = list(train_df_o[['St_Month', 'St_Day']].itertuples(index=False, name=None))\n",
    "valid_df_o['St_Month_Day'] = list(valid_df_o[['St_Month', 'St_Day']].itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating target enconding map dictionary for St_Month column using mean, max, min, std, and median values of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_grouped_df_s = train_df_s.groupby('St_Month')['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median'])\n",
    "month_map_dict_s = month_grouped_df_s.to_dict()\n",
    "\n",
    "month_grouped_df_o = train_df_o.groupby('St_Month')['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median'])\n",
    "month_map_dict_o = month_grouped_df_o.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating target enconding map dictionary for St_Hour column using mean, max, min, std, and median values of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_grouped_df_s = train_df_s.groupby('St_Hour')['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median'])\n",
    "hour_map_dict_s = hour_grouped_df_s.to_dict()\n",
    "\n",
    "hour_grouped_df_o = train_df_o.groupby('St_Hour')['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median'])\n",
    "hour_map_dict_o = hour_grouped_df_o.to_dict()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating target enconding map dictionary for day of week column using mean, max, min, std, and median values of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayofweek_grouped_df_s = train_df_s.groupby('dayofweek')['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median'])\n",
    "dayofweek_map_dict_s = dayofweek_grouped_df_s.to_dict()\n",
    "\n",
    "dayofweek_grouped_df_o = train_df_o.groupby('dayofweek')['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median'])\n",
    "dayofweek_map_dict_o = dayofweek_grouped_df_o.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating target enconding map dictionary for week of year column using mean, max, min, std, and median values of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekofyear_grouped_df_s = train_df_s.groupby('weekofyear')['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median'])\n",
    "weekofyear_map_dict_s = weekofyear_grouped_df_s.to_dict()\n",
    "\n",
    "weekofyear_grouped_df_o = train_df_o.groupby('weekofyear')['Rental_Bicycles_Count'].agg(['mean', 'max', 'min', 'std', 'median'])\n",
    "weekofyear_map_dict_o = weekofyear_grouped_df_o.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the map dictionaries to create the target encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['mean', 'max', 'min', 'std', 'median']:\n",
    "\n",
    "    train_df_s[f'dayofyear_{k}_te'] = train_df_s['dayofyear'].map(dayofyear_map_dict_s[k])\n",
    "    valid_df_s[f'dayofyear_{k}_te'] = valid_df_s['dayofyear'].map(dayofyear_map_dict_s[k])\n",
    "\n",
    "    train_df_o[f'dayofyear_{k}_te'] = train_df_o['dayofyear'].map(dayofyear_map_dict_o[k])\n",
    "    valid_df_o[f'dayofyear_{k}_te'] = valid_df_o['dayofyear'].map(dayofyear_map_dict_o[k])\n",
    "\n",
    "    train_df_s[f'month_day_{k}_te'] = train_df_s['St_Month_Day'].map(month_day_map_dict_s[k])\n",
    "    valid_df_s[f'month_day_{k}_te'] = valid_df_s['St_Month_Day'].map(month_day_map_dict_s[k])\n",
    "\n",
    "    train_df_o[f'month_day_{k}_te'] = train_df_o['St_Month_Day'].map(month_day_map_dict_o[k])\n",
    "    valid_df_o[f'month_day_{k}_te'] = valid_df_o['St_Month_Day'].map(month_day_map_dict_o[k])\n",
    "\n",
    "    train_df_s[f'month_{k}_te'] = train_df_s['St_Month'].map(month_map_dict_s[k])\n",
    "    valid_df_s[f'month_{k}_te'] = valid_df_s['St_Month'].map(month_map_dict_s[k])\n",
    "\n",
    "    train_df_o[f'month_{k}_te'] = train_df_o['St_Month'].map(month_map_dict_o[k])\n",
    "    valid_df_o[f'month_{k}_te'] = valid_df_o['St_Month'].map(month_map_dict_o[k])\n",
    "\n",
    "    train_df_s[f'hour_{k}_te'] = train_df_s['St_Hour'].map(hour_map_dict_s[k])\n",
    "    valid_df_s[f'hour_{k}_te'] = valid_df_s['St_Hour'].map(hour_map_dict_s[k])\n",
    "\n",
    "    train_df_o[f'hour_{k}_te'] = train_df_o['St_Hour'].map(hour_map_dict_o[k])\n",
    "    valid_df_o[f'hour_{k}_te'] = valid_df_o['St_Hour'].map(hour_map_dict_o[k])\n",
    "\n",
    "    train_df_s[f'dayofweek_{k}_te'] = train_df_s['dayofweek'].map(dayofweek_map_dict_s[k])\n",
    "    valid_df_s[f'dayofweek_{k}_te'] = valid_df_s['dayofweek'].map(dayofweek_map_dict_s[k])\n",
    "\n",
    "    train_df_o[f'dayofweek_{k}_te'] = train_df_o['dayofweek'].map(dayofweek_map_dict_o[k])\n",
    "    valid_df_o[f'dayofweek_{k}_te'] = valid_df_o['dayofweek'].map(dayofweek_map_dict_o[k])\n",
    "\n",
    "    train_df_s[f'weekofyear_{k}_te'] = train_df_s['weekofyear'].map(weekofyear_map_dict_s[k])\n",
    "    valid_df_s[f'weekofyear_{k}_te'] = valid_df_s['weekofyear'].map(weekofyear_map_dict_s[k])\n",
    "\n",
    "    train_df_o[f'weekofyear_{k}_te'] = train_df_o['weekofyear'].map(weekofyear_map_dict_o[k])\n",
    "    valid_df_o[f'weekofyear_{k}_te'] = valid_df_o['weekofyear'].map(weekofyear_map_dict_o[k])\n",
    "\n",
    "redundant_columns = [\n",
    "    'St_Hour',  'St_Day', 'St_Month',\n",
    "    'St_Year', 'User_type', 'Datetime',\n",
    "    'dayofweek', 'weekofyear','dayofyear',\n",
    "    'St_Month_Day'\n",
    "]\n",
    "\n",
    "train_df_s.drop(columns=redundant_columns, inplace=True)\n",
    "valid_df_s.drop(columns=redundant_columns, inplace=True)\n",
    "\n",
    "train_df_o.drop(columns=redundant_columns, inplace=True)\n",
    "valid_df_o.drop(columns=redundant_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also add lag features in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    train_df_s[f'lag{i}_target'] = train_df_s['Rental_Bicycles_Count'].shift(i)\n",
    "    train_df_o[f'lag{i}_target'] = train_df_o['Rental_Bicycles_Count'].shift(i)\n",
    "\n",
    "train_df_s.dropna(inplace=True)\n",
    "train_df_o.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rental_Bicycles_Count</th>\n",
       "      <th>step</th>\n",
       "      <th>dayofyear_mean_te</th>\n",
       "      <th>dayofyear_max_te</th>\n",
       "      <th>dayofyear_min_te</th>\n",
       "      <th>dayofyear_std_te</th>\n",
       "      <th>dayofyear_median_te</th>\n",
       "      <th>month_day_mean_te</th>\n",
       "      <th>month_mean_te</th>\n",
       "      <th>hour_mean_te</th>\n",
       "      <th>...</th>\n",
       "      <th>lag1_target</th>\n",
       "      <th>lag2_target</th>\n",
       "      <th>lag3_target</th>\n",
       "      <th>lag4_target</th>\n",
       "      <th>lag5_target</th>\n",
       "      <th>lag6_target</th>\n",
       "      <th>lag7_target</th>\n",
       "      <th>lag8_target</th>\n",
       "      <th>lag9_target</th>\n",
       "      <th>lag10_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>56.0</td>\n",
       "      <td>11</td>\n",
       "      <td>90.291667</td>\n",
       "      <td>344.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>84.901171</td>\n",
       "      <td>57.5</td>\n",
       "      <td>90.291667</td>\n",
       "      <td>156.010417</td>\n",
       "      <td>263.433854</td>\n",
       "      <td>...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>40.0</td>\n",
       "      <td>12</td>\n",
       "      <td>90.291667</td>\n",
       "      <td>344.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>84.901171</td>\n",
       "      <td>57.5</td>\n",
       "      <td>90.291667</td>\n",
       "      <td>156.010417</td>\n",
       "      <td>297.797042</td>\n",
       "      <td>...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>38.0</td>\n",
       "      <td>13</td>\n",
       "      <td>90.291667</td>\n",
       "      <td>344.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>84.901171</td>\n",
       "      <td>57.5</td>\n",
       "      <td>90.291667</td>\n",
       "      <td>156.010417</td>\n",
       "      <td>342.973706</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>35.0</td>\n",
       "      <td>14</td>\n",
       "      <td>90.291667</td>\n",
       "      <td>344.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>84.901171</td>\n",
       "      <td>57.5</td>\n",
       "      <td>90.291667</td>\n",
       "      <td>156.010417</td>\n",
       "      <td>340.710764</td>\n",
       "      <td>...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>54.0</td>\n",
       "      <td>15</td>\n",
       "      <td>90.291667</td>\n",
       "      <td>344.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>84.901171</td>\n",
       "      <td>57.5</td>\n",
       "      <td>90.291667</td>\n",
       "      <td>156.010417</td>\n",
       "      <td>341.898110</td>\n",
       "      <td>...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rental_Bicycles_Count  step  dayofyear_mean_te  dayofyear_max_te  \\\n",
       "10                   56.0    11          90.291667             344.0   \n",
       "11                   40.0    12          90.291667             344.0   \n",
       "12                   38.0    13          90.291667             344.0   \n",
       "13                   35.0    14          90.291667             344.0   \n",
       "14                   54.0    15          90.291667             344.0   \n",
       "\n",
       "    dayofyear_min_te  dayofyear_std_te  dayofyear_median_te  \\\n",
       "10               3.0         84.901171                 57.5   \n",
       "11               3.0         84.901171                 57.5   \n",
       "12               3.0         84.901171                 57.5   \n",
       "13               3.0         84.901171                 57.5   \n",
       "14               3.0         84.901171                 57.5   \n",
       "\n",
       "    month_day_mean_te  month_mean_te  hour_mean_te  ...  lag1_target  \\\n",
       "10          90.291667     156.010417    263.433854  ...         54.0   \n",
       "11          90.291667     156.010417    297.797042  ...         56.0   \n",
       "12          90.291667     156.010417    342.973706  ...         40.0   \n",
       "13          90.291667     156.010417    340.710764  ...         38.0   \n",
       "14          90.291667     156.010417    341.898110  ...         35.0   \n",
       "\n",
       "    lag2_target  lag3_target  lag4_target  lag5_target  lag6_target  \\\n",
       "10         33.0         17.0         18.0          3.0          5.0   \n",
       "11         54.0         33.0         17.0         18.0          3.0   \n",
       "12         56.0         54.0         33.0         17.0         18.0   \n",
       "13         40.0         56.0         54.0         33.0         17.0   \n",
       "14         38.0         40.0         56.0         54.0         33.0   \n",
       "\n",
       "    lag7_target  lag8_target  lag9_target  lag10_target  \n",
       "10          7.0         28.0         23.0          26.0  \n",
       "11          5.0          7.0         28.0          23.0  \n",
       "12          3.0          5.0          7.0          28.0  \n",
       "13         18.0          3.0          5.0           7.0  \n",
       "14         17.0         18.0          3.0           5.0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 29198 entries, 10 to 29157\n",
      "Data columns (total 42 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Rental_Bicycles_Count  29198 non-null  float64\n",
      " 1   step                   29198 non-null  int32  \n",
      " 2   dayofyear_mean_te      29198 non-null  float64\n",
      " 3   dayofyear_max_te       29198 non-null  float64\n",
      " 4   dayofyear_min_te       29198 non-null  float64\n",
      " 5   dayofyear_std_te       29198 non-null  float64\n",
      " 6   dayofyear_median_te    29198 non-null  float64\n",
      " 7   month_day_mean_te      29198 non-null  float64\n",
      " 8   month_mean_te          29198 non-null  float64\n",
      " 9   hour_mean_te           29198 non-null  float64\n",
      " 10  dayofweek_mean_te      29198 non-null  float64\n",
      " 11  weekofyear_mean_te     29198 non-null  float64\n",
      " 12  month_day_max_te       29198 non-null  float64\n",
      " 13  month_max_te           29198 non-null  float64\n",
      " 14  hour_max_te            29198 non-null  float64\n",
      " 15  dayofweek_max_te       29198 non-null  float64\n",
      " 16  weekofyear_max_te      29198 non-null  float64\n",
      " 17  month_day_min_te       29198 non-null  float64\n",
      " 18  month_min_te           29198 non-null  float64\n",
      " 19  hour_min_te            29198 non-null  float64\n",
      " 20  dayofweek_min_te       29198 non-null  float64\n",
      " 21  weekofyear_min_te      29198 non-null  float64\n",
      " 22  month_day_std_te       29198 non-null  float64\n",
      " 23  month_std_te           29198 non-null  float64\n",
      " 24  hour_std_te            29198 non-null  float64\n",
      " 25  dayofweek_std_te       29198 non-null  float64\n",
      " 26  weekofyear_std_te      29198 non-null  float64\n",
      " 27  month_day_median_te    29198 non-null  float64\n",
      " 28  month_median_te        29198 non-null  float64\n",
      " 29  hour_median_te         29198 non-null  float64\n",
      " 30  dayofweek_median_te    29198 non-null  float64\n",
      " 31  weekofyear_median_te   29198 non-null  float64\n",
      " 32  lag1_target            29198 non-null  float64\n",
      " 33  lag2_target            29198 non-null  float64\n",
      " 34  lag3_target            29198 non-null  float64\n",
      " 35  lag4_target            29198 non-null  float64\n",
      " 36  lag5_target            29198 non-null  float64\n",
      " 37  lag6_target            29198 non-null  float64\n",
      " 38  lag7_target            29198 non-null  float64\n",
      " 39  lag8_target            29198 non-null  float64\n",
      " 40  lag9_target            29198 non-null  float64\n",
      " 41  lag10_target           29198 non-null  float64\n",
      "dtypes: float64(41), int32(1)\n",
      "memory usage: 9.5 MB\n"
     ]
    }
   ],
   "source": [
    "train_df_s.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in ['mean', 'max', 'min', 'std', 'median']:\n",
    "#     test_df_s[f'dayofyear_{k}_te'] = test_df_s['dayofyear'].map(dayofyear_map_dict_s[k])\n",
    "#     test_df_o[f'dayofyear_{k}_te'] = test_df_o['dayofyear'].map(dayofyear_map_dict_o[k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
